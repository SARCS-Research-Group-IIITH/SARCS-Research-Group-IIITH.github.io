[
  {
    "id": "proj-imc",
    "slug": "in-memory-computing",
    "title": "In-Memory Computing",
    "shortDescription": "Developing novel computing paradigms that perform computation directly within memory arrays to overcome the von Neumann bottleneck. The  following repository contains a collection of literatures on SRAM-based in-memory computing (CIM) architectures and techniques.",
    "fullDescription": "In-memory computing (IMC) represents a paradigm shift in computer architecture by performing computations directly within memory arrays, eliminating costly data movement between memory and processing units. Our research focuses on developing efficient IMC architectures for machine learning inference workloads such as CNN, LLM, etc. Exploring both analog and digital implementations using CMOS-silicon DRAM/SRAM memories as well as emerging non-volatile memory technologies such as ReRAM, PCM, and MRAM.",
    "image": "/images/research/in-memory-computing.png",
    "topics": [
      "Digital SRAM In-Memory Computing",
      "Programmable Mixed-Signal SRAM In-Memory Computing",
      "ADC/DAC-free In-Memory Computing",
      "ReRAM-based In-Memory Computing"
    ],
    "links": {
      "github": "https://github.com/BUAA-CI-LAB/Literatures-on-SRAM-based-CIM"
    },
    "status": "active",
    "startYear": 2025
  },
  {
    "id": "proj-riscv",
    "slug": "riscv-architectures",
    "title": "RISC-V Architectures",
    "shortDescription": "Designing and implementing custom RISC-V processors and extensions for specialized computing applications.",
    "fullDescription": "RISC-V is an open-source instruction set architecture that enables innovation in processor design. Our lab develops custom RISC-V cores with specialized extensions for various application domains, including edge AI. We also contribute to the RISC-V ecosystem through tools, simulators, and educational resources. We also explore superscalar and out-of-order execution models along with vector and matrix processors to enhance performance.",
    "image": "/images/research/Riscv-architectures.png",
    "topics": [
      "Custom RISC-V core implementations",
      "Vector and matrix extension development",
      "Open-source processor development"
    ],
    "links": {
      "github": "https://github.com/riscv"
    },
    "status": "active",
    "startYear": 2025
  },
  {
    "id": "proj-accelerators",
    "slug": "hardware-accelerators",
    "title": "Hardware Accelerators",
    "shortDescription": "Building domain-specific accelerators for AI workloads. The following repository contains a deep learning accelerator based on systolic array architecture.",
    "fullDescription": "Domain-specific hardware accelerators provide orders of magnitude improvement in performance and energy efficiency compared to general-purpose processors. Our research develops accelerator architectures for applications like LLM training and inference, drawing inspiration from designs like Gemmini and Eyeriss. We explore novel dataflow architectures, including systolic arrays, memory hierarchies, and interconnects to optimize throughput and minimize latency. System level integration and software stack development are also key components of our work.",
    "image": "/images/research/systolic-array.png",
    "topics": [
      "Deep learning accelerator architectures",
      "Systolic array designs",
      "Interconnects",
      "Hardware-software co-design",
      "Compiler optimizations for accelerators"
    ],
    "links": {
      "github": "https://github.com/ucb-bar/gemmini"
    },
    "status": "active",
    "startYear": 2025
  }
  ,{
    "id": "proj-quantum",
    "slug": "quantum-computing",
    "title": "Quantum Computing",
    "shortDescription": "Research on quantum computation, quantum machine learning, and circuit-level quantum AI accelerators.",
    "fullDescription": " Quantum computing represents a new computing paradigm that exploits quantum mechanics principles such as superposition and entanglement to perform information processing beyond classical limits. Our research focuses on the design and simulation of quantum circuits and algorithms, with particular emphasis on quantum machine learning techniques for accelerating classical ML workloads and circuit-level approaches toward quantum AI accelerators. The work adopts a hardware-algorithm co-design perspective, leveraging hybrid quantum-classical methods and modern quantum software frameworks to explore scalable, application driven quantum computation.",
    "image": "/images/research/Quantum-Computing.jpeg",
    "topics": [
      "Quantum circuits and gate level design",
      "Quantum machine learning for classical ML acceleration",
      "Quantum AI and accelerator-oriented circuit design"
    ],
    "links": {},
    "status": "active",
    "startYear": 2025
  }
]
